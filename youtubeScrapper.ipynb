{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtubeScrapper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEnth2rfLNoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "119dd1d2-d167-4a70-c237-45f61ca5524f"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "import json\n",
        "filename = \"youtubeFile.csv\"\n",
        "\n",
        "with open(filename, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Title', 'Views', 'Date_Of_Release', 'URL'])\n",
        "\n",
        "    with open('data.txt') as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "        for url in data['link']:\n",
        "            code = requests.get(url['url']).text\n",
        "            soup = BeautifulSoup(code, \"lxml\")\n",
        "            views = soup.find(\"meta\", itemprop = \"interactionCount\")\n",
        "            date = soup.find(\"meta\", itemprop = \"datePublished\")\n",
        "            Title = soup.title.text\n",
        "            View = views.get('content')\n",
        "            Date = date.get('content')          \n",
        "            writer.writerow([Title, View, Date, url['url']])\n",
        "            \n",
        "print(\"SUCCESS!!\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}